{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Synchronous policy evaluation"
      ],
      "metadata": {
        "id": "6mXCE8RjcUOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m,n=5,5\n",
        "S=[(r,c) for r in range (m) for c in range (n)]\n",
        "A=['north', 'south', 'east', 'west']\n",
        "acount=len(A)\n",
        "terminal=[(4,4)]\n",
        "gamma=1"
      ],
      "metadata": {
        "id": "dcjP5qinc72l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transition(s,a, s_next):\n",
        "    r,c=s\n",
        "    if a=='north':\n",
        "      r=r if r==0 else r-1\n",
        "    if a=='south':\n",
        "      r=r if r==4 else r+1\n",
        "    if a=='east':\n",
        "      c=c if c==4 else c+1\n",
        "    if a=='west':\n",
        "      c=c if c==0 else c-1\n",
        "    prob=1 if (r, c)==s_next else 0\n",
        "    reward = -1\n",
        "    return prob, reward\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "dR8QwoY8d7x9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pi={s:{a:0.25 for a in A} for s in S}\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "IzD_MhzBpS_0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def to_numpy(v, m, n):\n",
        "    vnum = np.zeros((m, n))\n",
        "    for s in v:\n",
        "        vnum[s] = v[s]\n",
        "    return vnum\n",
        "\n",
        "theta = 1e-3  # Smaller convergence threshold for faster convergence\n",
        "gamma = 1  # Assuming a discount factor of 1\n",
        "delta = theta + 1\n",
        "v = {s: 0 for s in S}\n",
        "iter = 0\n",
        "\n",
        "# Assuming a random policy where pi[s][a] = 1/len(A) for all states and actions\n",
        "pi = {s: {a: 1/len(A) for a in A} for s in S}\n",
        "\n",
        "while delta >= theta:\n",
        "    iter += 1\n",
        "    v_next = {s: 0 for s in S}\n",
        "    delta = 0\n",
        "\n",
        "    for s in S:\n",
        "        if s in terminal:\n",
        "            continue  # Skip terminal states\n",
        "\n",
        "        # Evaluate the value for each action\n",
        "        for a in A:\n",
        "            total_value = 0  # Initialize the expected value for this action\n",
        "            for s_next in S:\n",
        "                prob, reward = transition(s, a, s_next)\n",
        "                total_value += prob * (reward + gamma * v[s_next])\n",
        "            v_next[s] += pi[s][a] * total_value  # Update value function based on policy\n",
        "\n",
        "        # Update delta to track the maximum change in value\n",
        "        delta = max(delta, abs(v_next[s] - v[s]))\n",
        "\n",
        "    v = v_next.copy()  # Update the value function for the next iteration\n",
        "\n",
        "# Round the value function for easier interpretation\n",
        "v = {s: np.round(v[s], 2) for s in S}\n",
        "\n",
        "# Print the number of iterations and the final value function\n",
        "print(f\"Number of iterations: {iter}\")\n",
        "print(\"Value function:\")\n",
        "print(v)\n",
        "\n",
        "# Convert the value function to a NumPy array and print\n",
        "v_array = to_numpy(v, m, n)\n",
        "print(v_array)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfDS7F6R-9Fh",
        "outputId": "6e2ec757-cec4-4169-aeda-f21ee7c122df"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iterations: 647\n",
            "Value function:\n",
            "{(0, 0): -106.73, (0, 1): -104.73, (0, 2): -101.29, (0, 3): -97.54, (0, 4): -95.0, (1, 0): -104.73, (1, 1): -102.17, (1, 2): -97.62, (1, 3): -92.33, (1, 4): -88.46, (2, 0): -101.29, (2, 1): -97.62, (2, 2): -90.67, (2, 3): -81.72, (2, 4): -74.05, (3, 0): -97.54, (3, 1): -92.33, (3, 2): -81.72, (3, 3): -65.84, (3, 4): -47.96, (4, 0): -95.0, (4, 1): -88.46, (4, 2): -74.05, (4, 3): -47.96, (4, 4): 0}\n",
            "[[-106.73 -104.73 -101.29  -97.54  -95.  ]\n",
            " [-104.73 -102.17  -97.62  -92.33  -88.46]\n",
            " [-101.29  -97.62  -90.67  -81.72  -74.05]\n",
            " [ -97.54  -92.33  -81.72  -65.84  -47.96]\n",
            " [ -95.    -88.46  -74.05  -47.96    0.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8mPLI9jQPzK",
        "outputId": "19029fe0-3d10-401b-b43a-d0222334038d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iterations: 419\n",
            "Value function:\n",
            "{(0, 0): -106.76, (0, 1): -104.77, (0, 2): -101.33, (0, 3): -97.57, (0, 4): -95.03, (1, 0): -104.77, (1, 1): -102.21, (1, 2): -97.65, (1, 3): -92.37, (1, 4): -88.49, (2, 0): -101.33, (2, 1): -97.65, (2, 2): -90.7, (2, 3): -81.75, (2, 4): -74.07, (3, 0): -97.57, (3, 1): -92.37, (3, 2): -81.75, (3, 3): -65.86, (3, 4): -47.98, (4, 0): -95.03, (4, 1): -88.49, (4, 2): -74.07, (4, 3): -47.98, (4, 4): 0}\n",
            "[[-106.76 -104.77 -101.33  -97.57  -95.03]\n",
            " [-104.77 -102.21  -97.65  -92.37  -88.49]\n",
            " [-101.33  -97.65  -90.7   -81.75  -74.07]\n",
            " [ -97.57  -92.37  -81.75  -65.86  -47.98]\n",
            " [ -95.03  -88.49  -74.07  -47.98    0.  ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def to_numpy(v, m, n):\n",
        "    vnum = np.zeros((m, n))\n",
        "    for s in v:\n",
        "        vnum[s] = v[s]\n",
        "    return vnum\n",
        "\n",
        "theta = 1e-3  # Convergence threshold\n",
        "gamma = 1  # Discount factor\n",
        "delta = theta + 1\n",
        "v = {s: 0 for s in S}  # Initialize value function to 0 for all states\n",
        "iter = 0\n",
        "\n",
        "# Assuming a random policy where pi[s][a] = 1/len(A) for all states and actions\n",
        "pi = {s: {a: 1/len(A) for a in A} for s in S}\n",
        "\n",
        "while delta >= theta:\n",
        "    iter += 1\n",
        "    delta = 0\n",
        "\n",
        "    for s in S:\n",
        "        if s in terminal:\n",
        "            continue  # Skip terminal states\n",
        "\n",
        "        # Save the current value of the state for delta calculation later\n",
        "        v_old = v[s]\n",
        "\n",
        "        # Initialize new value for state s\n",
        "        new_value = 0\n",
        "\n",
        "        # Iterate over all actions\n",
        "        for a in A:\n",
        "            total_value = 0  # Expected value for the action a\n",
        "\n",
        "            # Iterate over all possible next states\n",
        "            for s_next in S:\n",
        "                prob, reward = transition(s, a, s_next)\n",
        "                total_value += prob * (reward + gamma * v[s_next])\n",
        "\n",
        "            # Update the value of the state s based on the current policy\n",
        "            new_value += pi[s][a] * total_value\n",
        "\n",
        "        # In-place update of the value function\n",
        "        v[s] = new_value\n",
        "\n",
        "        # Update delta to track the maximum change in value\n",
        "        delta = max(delta, abs(v[s] - v_old))\n",
        "\n",
        "# Round the value function for easier interpretation\n",
        "v = {s: np.round(v[s], 2) for s in S}\n",
        "\n",
        "# Print the number of iterations and the final value function\n",
        "print(f\"Number of iterations: {iter}\")\n",
        "print(\"Value function:\")\n",
        "print(v)\n",
        "\n",
        "# Convert the value function to a NumPy array and print\n",
        "v_array = to_numpy(v, m, n)\n",
        "print(v_array)\n"
      ]
    }
  ]
}